# 机器学习算法评价指标

## 准确率、精准率和召回率
TP：样本为正，预测结果为正；

FP：样本为负，预测结果为正；

TN：样本为负，预测结果为负；

FN：样本为正，预测结果为负。

准确率、精准率和召回率的计算公式如下：

准确率（accuracy）： (TP + TN )/( TP + FP + TN + FN)

精准率（precision）：TP / (TP + FP)，正确预测为正占全部预测为正的比例

召回率（recall）： TP / (TP + FN)，正确预测为正占全部正样本的比例

## ROC和AUC

### ROC曲线简介
ROC曲线则是从阈值选取角度出发来研究学习器泛化性能的有力工具。

若我们更重视“查准率”，则可以把阈值设置的大一些，让分类器的预测结果更有把握；若我们更重视“查全率”，则可以把阈值设置的小一些，让分类器预测出更多的正例。


ROC曲线的纵轴是“真正例率”(True Positive Rate, 简称TPR)，横轴是“假正例率”(False Positive Rate,简称FPR)。

### ROC曲线的意义
- ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响。
- 有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的准确性就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少。
- 可以对不同的学习器比较性能。将各个学习器的ROC曲线绘制到同一坐标中，直观地鉴别优劣，靠近左上角的ROC曲所代表的学习器准确性最高。

### AUC

ROC曲线下的面积，即AUC(Area Under ROC Curve)。AUC就是ROC曲线下的面积，衡量学习器优劣的一种性能指标。
AUC是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。

ROC曲线用在多分类中是没有意义的。

- AUC ＝ 1，代表完美分类器
- 0.5 < AUC < 1，优于随机分类器
- 0 < AUC < 0.5，差于随机分类器

## TPR和FPR
考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。

TP：正确肯定的数目；

FN：漏报，没有正确找到的匹配的数目；

FP：误报，给出的匹配是不正确的；

TN：正确拒绝的非匹配对数；

真正类率(true positive rate ,TPR), 计算公式为TPR=TP/ (TP+ FN)，刻画的是分类器所识别出的 正实例占所有正实例的比例。

另外一个是假正类率(false positive rate, FPR),计算公式为FPR= FP / (FP + TN)，计算的是分类器错认为正类的负实例占所有负实例的比例。

## KS值

KS 值表示了模型区分好坏客户的能力。KS 的取值范围在0.5和1之间，值越大，模型的预测准确性越好。一般，KS > 0.4 即认为模型有比较好的预测性能。

## F1-Score

Precision和Recall指标有时是此消彼长的，即精准率高了，召回率就下降，在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均。

## 欠拟合和过拟合

训练集和验证集准确率都很低，很可能是欠拟合。解决欠拟合的方法就是增加模型参数，比如，构建更多的特征，减小正则项。

训练集和验证集准确率相差太多。解决过拟合的方法有增大训练集或者降低模型复杂度，比如增大正则项，或者通过特征选择减少特征数。

