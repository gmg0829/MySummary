## 机器学习
现实问题抽象为数学问题，机器解决数学问题从而解决现实问题。

## 决策树
决策树是一种逻辑简单的机器学习算法，它是一种树形结构，所以叫决策树。
决策树是一种解决分类问题的算法,决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成：
- 根节点：包含样本的全集
- 内部节点：对应特征属性测试
- 叶节点：代表决策的结果

预测时，在树的内部节点处用某一属性值进行判断，根据判断结果决定进入哪个分支节点，直到到达叶节点处，得到分类结果。这是一种基于 if-then-else 规则的有监督学习算法，决策树的这些规则通过训练得到。

流程：
- 特征选择
特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。
- 决策树生成
从根节点触发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。
- 决策树剪枝
剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。
### 3 种典型的决策树算法

- ID3 算法
- C4.5 算法
- CART（Classification and Regression Tree）
## 随机森林
随机森林是由很多决策树构成的，不同决策树之间没有关联。
每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。


## 逻辑回归

逻辑回归（Logistic Regression）主要解决二分类问题，用来表示某件事情发生的可能性。

容易欠拟合，一般准确度不太高。只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分。

## 线性回归

回归的目的是为了预测，比如预测明天的天气温度，预测股票的走势。回归之所以能预测是因为他通过历史数据，摸透了“套路”，然后通过这个套路来预测未来的结果。

线性关系不仅仅只能存在 2 个变量（二维平面）。3 个变量时（三维空间），线性关系就是一个平面，4 个变量时（四维空间）。

![](
  ./line-logic.png)

## 聚类分析
聚类分析指将物理或抽象对象的集合分组为由类似的对象组成的多个类的分析过程。衡量不同数据源间的相似性，以及把数据源分类到不同的簇中。





