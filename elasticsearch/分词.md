## 分词

分词是将文本转换成一系列单词（Term or Token）的过程，也可以叫文本分析，在ES里面称为Analysis。

## 分词器

分词器是ES中专门处理分词的组件，英文为Analyzer，它的组成如下：

- Character Filters：针对原始文本进行处理，比如去除html标签
- Tokenizer：将原始文本按照一定规则切分为单词
- Token Filters：针对Tokenizer处理的单词进行再加工，比如转小写、删除或增新等处理

### 预定义的分词器
ES自带的分词器有如下：

- Standard Analyzer

    - 默认分词器
    - 按词切分，支持多语言
    - 小写处理
- Simple Analyzer

    - 按照非字母切分
    - 小写处理
- Whitespace Analyzer

    - 空白字符作为分隔符
- Stop Analyzer

    - 相比Simple Analyzer多了去除请用词处理
    - 停用词指语气助词等修饰性词语，如the, an, 的， 这等
- Keyword Analyzer

    - 不分词，直接将输入作为一个单词输出
- Pattern Analyzer
    - 通过正则表达式自定义分隔符
    - 默认是\W+，即非字词的符号作为分隔符
- Language Analyzer
   - 提供了30+种常见语言的分词器

### 中文分词

- 难点
    - 中文分词指的是将一个汉字序列切分为一个一个的单独的词。在英文中，单词之间以空格作为自然分界词，汉语中词没有一个形式上的分界符
    - 上下文不同，分词结果迥异，比如交叉歧义问题
常见分词系统
- IK：实现中英文单词的切分，可自定义词库，支持热更新分词词典
- jieba：支持分词和词性标注，支持繁体分词，自定义词典，并行分词等
- Hanlp：由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用
- THUAC：中文分词和词性标注

## 参考

https://juejin.im/post/5b799cf551882542f676daba
